{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Ever wonder who had the heaviest offensive line in the NFL? Or which team had the most penalties over the past few seasons? What if I told you that you didn't need to be an ESPN statistician to figure this out? What if---after a minimal investment in setup---you could easily query NFL data and start building your own predictive models?\n",
    "\n",
    "Well I'm here to tell you that it is actually pretty easy to do---even for non-coders. After a few days of tinkering I was able to establish an easy to use, mostly accurate pipeline for analyzing NFL data. This post is directed towards people of all skill levels, but particularly novices.\n",
    "\n",
    "What you'll need:\n",
    "* Willingness to learn new things and to grind out the many small issues that inevitably arise\n",
    "* A computer running MacOS (or any other Unix based system)\n",
    "* Python\n",
    "* nfldb\n",
    "* PostgreSQL \n",
    "\n",
    "## Python\n",
    "Python is a powerful yet easy to use programming language capable of processing and analyzing data. It is widely used throughout the industry and is pre-installed on MacOS (although we will be using Python 3). There are about a million different tutorials you can use to become more familiar with how Python is used; I recommend [Python the Hard Way](https://learnpythonthehardway.org). Interactive tutorials are available at [Datacamp](https://www.datacamp.com/learn-python-with-anaconda?utm_source=Anaconda_download&utm_campaign=datacamp_training&utm_medium=banner).\n",
    "\n",
    "Although Python is pre-installed on MacOS, I recommend installing it via [Anaconda](https://www.anaconda.com). Anaconda is a convenient bundle of Python and many of the additional libraries necessary to do data analysis. I will be using Python 3, so be sure to download that version (note that the download may take a while).\n",
    "\n",
    "Python code can be edited and run either by the command line or an interactive development environment (IDE). I highly recommend using an IDE as they are significanlty more useful in working with code. Anaconda comes with **Jupyter** (what I am using to write and publish this article). Jupyter uses your web browser to allow you to edit and execute your code in chunks. However, I use [Spyder](https://pythonhosted.org/spyder/) to develop my code because it allows me to see variables, dataframes, arrays, etc. that I have generated. Once installed type either `jupyter notebook` or `spyder` in terminal and a new session will begin.\n",
    "\n",
    "## SQL\n",
    "\n",
    "SQL (structured query language) is an ancient method of querying databases. It's super easy to figure out once you see it in practice. The bulk of this post will be a series of queries designed to illustrate the basic functionality of SQL.\n",
    "\n",
    "## PostgreSQL\n",
    "\n",
    "PostgreSQL is a database management system that allows you to store and query your data using SQL. There are several ways to install PostgreSQL. You can either use the [Homebrew](https://brew.sh) method or [Postgres App](http://postgresapp.com). The Postgres App is the easiest option for MacOS. I am currently using [Postico](https://eggerapps.at/postico/) to view my data and test out different queries.\n",
    "\n",
    "\n",
    "## NFLDB\n",
    "Once you have PostgreSQL setup, you are ready to get the NFL data. [nfldb](https://github.com/BurntSushi/nfldb) is a module designed to scrape data from the NFL website and put it into a SQL table. The [instructions](https://github.com/BurntSushi/nfldb/wiki/Installation) for installing nfldb are overly complicated, so here is a much more simple guide. Basically, all you need to do is create a blank Postgres datbase, run the SQL file, and then make a minor configuration change. \n",
    "\n",
    "The following is a list of instruction you will need to type into your command line. On Mac, simply open terminal and make sure you are in your `~` (aka your root directory). If you are unsure what the name of your `~` directory is, open finder and look at the name next to the house icon. Your terminal should open to `~` by default.\n",
    "\n",
    "1. Open terminal and type `psql` to launch PostgreSQL\n",
    "2. Type `CREATE DATABASE nfldb;`, then `\\connect nfldb`, and then `CREATEUSER nfldb;`\n",
    "3. Download the [SQL file](http://burntsushi.net/stuff/nfldb/nfldb.sql.zip) and unzip it to your `~` directory\n",
    "4. Make sure terminal is in your `~` directory and type `psql -U nfldb nfldb < nfldb.sql`. This should create an empty NFL database in PostgreSQL\n",
    "5. Now type `pip install nfldb` to install the Python code necessary to fill in the NFL database\n",
    "6. Now type `locate config.ini.sample`. It should return a path like `/usr/local/share/nfldb/config.ini.sample`. Highlight this path and copy it\n",
    "7. Type `mkdir -p $HOME/.config/nfldb` and press enter\n",
    "8. Type `cp <paste the path from step 6 here> $HOME/.config/nfldb/config.ini`\n",
    "\n",
    "And that's it! Everything should work now. The [installation guide](https://github.com/BurntSushi/nfldb/wiki/Installation) has a few exercises to make sure everything is up and running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the Data\n",
    "\n",
    "First thing we need to do is import the libraries we will be using. **Pandas** is an incredibly useful library for manipulating data. Here we will use the `pandas.read_sql` method to query the data and display the results in a nice table format. We will also be using **psycopg2** to create a connection to the PostgreSQL database.\n",
    "\n",
    "If for some reason neither pandas nor psycog2 is not installed, use `pip <package name>` or `conda install <package name>` to perform the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:26.051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import psycopg2\n",
    "import psycopg2\n",
    "\n",
    "# provide your credentials; yours should look the same except with you user name\n",
    "conn=psycopg2.connect(\"dbname='nfldb' user='kthomas1' host='localhost' password='' port=5432\")\n",
    "\n",
    "# create the connection\n",
    "cur=conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT, FROM, WHERE, LIMIT\n",
    "\n",
    "Here we will perform the most basic query. Let's look at the first 10 players for the Carolina Panthers listed in the `player` table. First we `SELECT` the variables we want `FROM` the `player` table. We use the `WHERE` command to filter to just the Panthers. Finally, we only want the first 10 observations, so  type `LIMIT 10` at the end. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:42:57.318Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"SELECT full_name, position, uniform_number, height, weight, college \n",
    "FROM player \n",
    "WHERE team='CAR' \n",
    "LIMIT 10\"\"\",con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we see the first limitation of the NFLDB dataset --- it is outdated. Several of these players are not on the Carolina roster as of 2017. I have tried updating the database but have been unsuccessful. I will update this post if I can determine a solution.\n",
    "\n",
    "One of the nice things about Jupyer + pandas are that these dataframes have sortable columns.\n",
    "\n",
    "## COUNT, AVG, SUM, MAX, MIN\n",
    "\n",
    "SQL allows us to count the number of entries that meet a certain criteria. We can also take the sum or average or find the max/min of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:42:58.663Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of posessions by Carolina from 2009-2016\n",
    "cur.execute(\"\"\"SELECT COUNT(pos_team)\n",
    "FROM play\n",
    "WHERE pos_team='CAR'\"\"\")\n",
    "\n",
    "pos,=cur.fetchall()\n",
    "print(\"Total Number of Possessions by Carolin from 2009-2016: {:,}\".format(int(pos[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:42:59.406Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average penalty yards for buffalo\n",
    "cur.execute(\"\"\"SELECT AVG(penalty_yds)\n",
    "FROM play\n",
    "WHERE pos_team='BUF' AND penalty_yds!=0\"\"\")\n",
    "\n",
    "penalty,=cur.fetchall()\n",
    "print(\"Average Penalty Yards for Buffalo: {:.2f}\".format(float(penalty[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:42:59.892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total weight of offensive line for oakland\n",
    "cur.execute(\"\"\"SELECT sum(weight)\n",
    "FROM player\n",
    "WHERE team='OAK' AND (position='OL' OR position='OG' OR position='OT' OR position='C')\"\"\")\n",
    "\n",
    "weight,=cur.fetchall()\n",
    "print(\"Total Weight of the Oakland offensive line: {:,} lbs\".format(int(weight[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:00.318Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the shortest player in the database\n",
    "cur.execute(\"\"\"SELECT min(height)\n",
    "FROM player\n",
    "WHERE status='Active'\"\"\")\n",
    "\n",
    "height,=cur.fetchall()\n",
    "height_feet = float(height[0])/12\n",
    "print(\"Height of the shortest player in the database: {:.1f} ft\".format(float(height_feet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORDER BY\n",
    "\n",
    "If we want to sort our queries we can use ORDER BY. Here we get a list of the 10 heaviest players in the database by using a descending order by on weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:01.268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"select full_name, position, team, weight\n",
    "from player\n",
    "where status='Active'\n",
    "order by weight DESC\n",
    "limit 10\"\"\",con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTINCT, GROUP BY, CHAR_LENGTH\n",
    "\n",
    "When exploring a database it is often useful to see the unique values in a column. SQL allows us to do that by using the DISTINCT command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:02.839Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all the positions from the player db\n",
    "pd.read_sql(\"\"\"select DISTINCT position\n",
    "from player\"\"\",con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out who had the most penalty yards from 2009-2016, we'll need to combine SUM, GROUP BY, and then ORDER BY. The query below essentially sums the every entry in the `penalty_yds` column for each team. We also use the AS operator to rename the `sum(penalty_yds)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:03.656Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# who had the most penalty yards for 2009-2016?\n",
    "pd.read_sql(\"\"\"select pos_team, sum(penalty_yds) as total_penalty_yards\n",
    "from play\n",
    "group by pos_team\n",
    "order by sum(penalty_yds) DESC\"\"\",con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every play in the database is accompanied by a description of what happened on that play. If we wanted to find out which play has the longest description we can use the CHAR_LENGTH function along with ORDER BY DESC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:04.486Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# play that have the longest description\n",
    "\n",
    "cur.execute(\"\"\"select description\n",
    "from play\n",
    "order by char_length(description) DESC\n",
    "limit 1\"\"\")\n",
    "\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've read through that play description a few times and I still don't know what happened ¯\\\\_| ಠ ∧ ಠ |_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINS\n",
    "\n",
    "Joining data is one of the most important functions of SQL but it is also the one most likely to lead to confusion. There are many [resources](https://www.tutorialspoint.com/postgresql/postgresql_using_joins.htm) on SQL joins, but here we will just give a basic example. \n",
    "\n",
    "Our queries so far have used the entire dataset. For example, the table on most penalty yards was from all years in the `play` dataset. If we wanted to filter to just one year we would need to bring in the year from the `game` dataset. This would require us to join the `game` dataset with the `play` dataset.\n",
    "\n",
    "When joining tables, each variable that is selected needs to be prefaced by the name of the table it is coming from followed by a \".\". For example, selecting `pos_team` from the `play` dataset would be written as `play.pos_team`. \n",
    "\n",
    "This is a very simple join and as such we can match in the tables simply using a WHERE clause that matches the shared game ids (`gsis_id`) between the two tables. We then filter to the year 2015 and the regular season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:08.958Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# most penalty yards in 2015; need to join play to game and filter to regular season\n",
    "pd.read_sql(\"\"\"select play.pos_team, sum(play.penalty_yds)\n",
    "from play, game where play.gsis_id=game.gsis_id and game.season_year=2015 and game.season_type='Regular'\n",
    "group by play.pos_team\n",
    "order by sum(play.penalty_yds) DESC\n",
    "limit 10\"\"\",con=conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH SUBQUERIES\n",
    "\n",
    "This is a slightly more advanced topic. WITH statements are a powerful tool that allows you to create sub tables and then join them together. For example, if wanted additional aggregate statistics for each drive not including in the drives dataset, we would have to aggregate these numbers from the play table to the drive level then join it to the drive table. The WITH statement allows us to do this in one query. \n",
    "\n",
    "For clarity is an example. Let's say we want the following table:\n",
    "\n",
    "| game_id | drive_id | yards_gained | passing_first_downs |\n",
    "|---------|----------|--------------|---------------------|\n",
    "| 1       | 1        | 40           | 2                   |\n",
    "| 1       | 2        | 70           | 3                   |\n",
    "| 1       | 3        | 20           | 0                   |\n",
    "\n",
    "We would have to join the drive table:\n",
    "\n",
    "| game_id | drive_id | yards_gained |\n",
    "|---------|----------|--------------|\n",
    "| 1       | 1        | 40           |\n",
    "| 1       | 2        | 70           |\n",
    "| 1       | 3        | 20           |\n",
    "\n",
    "With the play table (here is a truncated example --- imagine the passing_first_down column sums to 2 where drive_id==1):\n",
    "\n",
    "| game_id | drive_id | play_id | passing_first_down |\n",
    "|---------|----------|---------|--------------------|\n",
    "| 1       | 1        | 1       | 0                  |\n",
    "| 1       | 1        | 2       | 1                  |\n",
    "| 1       | 1        | 3       | 0                  |\n",
    "\n",
    "We would take the sum of passing_first_down across game_id and drive_id and then link the drive table and the play table on game_id and drive_id.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-03T19:43:10.052Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_sql(\"\"\"with plays as\n",
    "\t(select gsis_id, drive_id, sum(passing_first_down) as pass_first, sum(penalty_first_down) as penalty_first, sum(rushing_first_down) as rushing_first, sum(first_down) as first, sum(third_down_conv) as third\n",
    "\tfrom play\n",
    "\tgroup by gsis_id, drive_id),\n",
    "drives as\n",
    "\t(select gsis_id, drive_id, pos_team, result, start_field, pos_time, penalty_yards, yards_gained, play_count\n",
    "\tfrom drive)\n",
    "select * from drives\n",
    "inner join plays on (drives.gsis_id=plays.gsis_id and drives.drive_id=plays.drive_id)\n",
    "limit 10\"\"\",con=conn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
