
<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

  <!-- <link rel="stylesheet" type="text/css" href="./theme/pygments/github.min.css"> -->
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">





  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />


<meta name="author" content="kyle thomas" />
<meta name="description" content="Chain of Thought: starting with regularization techniques of ridge regression and lasso we move to what is a norm while covering optimization, Moore-Penrose Pseudoinverse matrix, and Lagrangian multiplier." />
<meta name="keywords" content="linear-algebra, regression, math">
<meta property="og:site_name" content="cmd+build"/>
<meta property="og:title" content="Chain of Thought: Ridge Regression, Norms, and Optimization"/>
<meta property="og:description" content="Chain of Thought: starting with regularization techniques of ridge regression and lasso we move to what is a norm while covering optimization, Moore-Penrose Pseudoinverse matrix, and Lagrangian multiplier."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./cot-norm.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2017-02-08 00:00:00-05:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/kyle-thomas.html">
<meta property="article:section" content="tech"/>
<meta property="article:tag" content="linear-algebra"/>
<meta property="article:tag" content="regression"/>
<meta property="article:tag" content="math"/>
<meta property="og:image" content="">

  <title>cmd+build &ndash; Chain of Thought: Ridge Regression, Norms, and Optimization</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="./theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href="."></a></h1>


      <ul class="social">
        <li><a class="sc-github" href="https://github.com/tkthomas27" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-linkedin" href="www.linkedin.com/in/timothykylethomas" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a class="sc-twitter" href="https://twitter.com/tkylethomas27" target="_blank"><i class="fa fa-twitter"></i></a></li>
      </ul>
    

      <nav>
        <ul class="list">

          <li><a href="/about.html" target="_blank">About</a></li>
          <li><a href="/archives.html" target="_blank">Archives</a></li>
          <li><a href="/categories.html" target="_blank">Categories</a></li>
          <li><a href="/tags.html" target="_blank">Tags</a></li>
        </ul>
      </nav>

  </div>



  </aside>
  <main>


<article class="single">
  <header>
    <h1 id="cot-norm">Chain of Thought: Ridge Regression, Norms, and Optimization</h1>
    <p>
          Posted on Wed 08 February 2017 in <a href="./category/tech.html">tech</a>


    </p>
  </header>


  <div>
    <h1>1 Ridge Regression and the Lasso</h1>
<p>While working through <a href="http://timothykylethomas.me/islr.html#islr">Introduction to Statistical Learning</a> (ISLR), I became intrigued by the lecture on <a href="https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/model_selection.pdf">subset selection</a>. Subset selection is the process of reducing linear models to only those features (viz. independent variables) that are most relevant. Old standbys in this field are piecewise forward and backward selection. The hot topic now, however, are the shrinkage methods ridge regression and lasso.</p>
<p>This two methods are popular now because of the explosion of higher dimensional data (i.e., where there are more features than observations). Piecewise selection frequently becomes intractable where there are many features. Shrinkage methods solve this method by building in a penalty to OLS estimation that automatically pushes coefficients to zero (you should read the above notes on subset selection for more details, here we are just plowing ahead with my chain of thought). The point of having less features and lower coefficients is the reduction in the overall variance of the model.</p>
<p>Let's briefly overview OLS estimation and the changes the ridge and lasso make. With a vector <span class="math">\(b\)</span> of responses and a matrix <span class="math">\(A\)</span> of features, we seek to find the vector <span class="math">\(x\)</span> that makes <span class="math">\(Ax = b\)</span>.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup> Using standard terms for OLS, <span class="math">\(b\)</span> is the dependent variable <span class="math">\(y\)</span>, <span class="math">\(A\)</span> is our observations <span class="math">\(X\)</span>, and <span class="math">\(x\)</span> is the vector of <span class="math">\(\beta\)</span>s giving use the formula <span class="math">\(X \beta = y\)</span> (note that the order of <span class="math">\(X\)</span> and <span class="math">\(\beta\)</span> is important). The simplest way to express this is from a geometric perspective: we want to choose a <span class="math">\(\beta\)</span> vector that minimizes the distance between <span class="math">\(X\)</span> and <span class="math">\(y\)</span> (the bars <span class="math">\(||\)</span> tell us that this is a  euclidean norm --- a fancy way of saying a distance):</p>
<div class="math">$$\hat{\beta} = \min_{\beta} ||y - X\beta||. $$</div>
<p>The OLS way to do this is to minimize the sum of the squared residuals (RSS). There are many ways to express the minimization of RSS (a la the geometric version above), but here we will use the version presented in ISLR to remain consistent.</p>
<p>First, the RSS we seek to minimize is:</p>
<div class="math">$$ RSS = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 .$$</div>
<p>This is probably the most complicated way to present OLS, but in doing so, it quite exactly tells you what is happening.</p>
<p>For ridge regression we add a shrinkage penalty with tuning parameter <span class="math">\(\lambda\)</span> to the criterion:</p>
<div class="math">$$ RSS + \lambda \sum_{j=1}^p \beta^2_j .$$</div>
<p>Now, instead of just trying to minimize RSS, we are trying to minimize a larger number: RSS plus the shrinkage penalty. This will force the model to prefer <span class="math">\(\beta\)</span>s that are smaller and closer to zero thus helping to reduce the variance of the model (you will often end up with very small coefficients and not ones exactly equal to zero).</p>
<p>The <span class="math">\(\sum \beta^2\)</span> part of the penalty is also known as the <span class="math">\(\ell_2\)</span> penalty and can be written in its geometric form as <span class="math">\(||\beta|\vert_2\)</span>.</p>
<p>A newer sibling of the ridge regression that is becoming increasingly more popular is the lasso:</p>
<div class="math">$$ RSS + \lambda \sum_{j=1}^p |\beta_j |$$</div>
<p>By recasting the penalty as an <span class="math">\(\ell_1\)</span> norm (alternatively <span class="math">\(||\beta|\vert_1\)</span>), the minimization can take coefficients to zero (not just close to zero as in ridge regression). The result is feature selection: only those coefficients that are non-zero are kept. The disadvantage of the lasso is that it requires computationally intensive numerical searching algorithms.</p>
<p>So the difference between ridge and lasso is the nature of this penalty: ridge uses the <span class="math">\(\ell_2\)</span> norm while the lasso uses the <span class="math">\(\ell_1\)</span> norm (the <span class="math">\(\LaTeX\)</span> code for <span class="math">\(\ell\)</span> is <code>\ell</code>). So what are these norms?</p>
<h1>2 Norm</h1>
<p>When researching norms --- particularly in the context of optimization problems like those used in subset selection --- I came across this helpful <a href="https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/">blog post</a>.</p>
<p>There is a lot to unpack from this post and many avenues of thoughts. One could easily start exploring the nature of Euclidean space and its alternatives (i.e., <a href="https://en.wikipedia.org/wiki/Hyperbolic_space">hyperbolic space</a>) and the variety of <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norms</a>. However, I decided to keep to the <span class="math">\(\ell_1\)</span> and <span class="math">\(\ell_2\)</span> norms, their optimization, and their application to the statistical issues presented above.</p>
<p>A simple way to think of a norm is a distance or size in a p-dimensional space. As the author of the blog writes, the general form of a norm is:</p>
<div class="math">$$||x|\vert_p = \sqrt[p]{\sum_i |x_i|^p} $$</div>
<p>The <span class="math">\(\ell_1\)</span> norm ends up simply being the sum of the absolute value of each element, while the <span class="math">\(\ell_2\)</span> norm is the square root of the sum of the squared elements.</p>
<p>You may remember from grade school the formula for the distance (<span class="math">\(d\)</span>) between two points on the 2-dimensional plane <span class="math">\((x_1,y_2)\)</span> and <span class="math">\((x_2,y_2)\)</span>:</p>
<div class="math">$$ d = \sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2} $$</div>
<p>Which we can rewrite as:</p>
<div class="math">$$|| y - x|\vert_2 = \sqrt{\sum (y - x)^2} $$</div>
<p>Does the above remind you of our minimization problem <span class="math">\(\hat{\beta} = \min_{\beta} ||y - X\beta||\)</span> or the RSS formula? It should because, as stated above, the OLS optimization is simply trying to find the <span class="math">\(\beta\)</span>s that minimizes the distance between <span class="math">\(X\)</span> and <span class="math">\(y\)</span>. In other words, OLS wants the vector of <span class="math">\(\beta\)</span>s that makes the <span class="math">\(\ell_2\)</span> norm <span class="math">\(||y - X\beta||\)</span> the smallest. OLS is just a more multi-faceted version of something you have been doing all along.</p>
<h1>3 Norm Optimization</h1>
<p>To tie it together: regular OLS is simply minimizing an <span class="math">\(\ell_2\)</span> norm, ridge regression is minimizing an <span class="math">\(\ell_2\)</span> norm plus an <span class="math">\(\ell_2\)</span> penalty, and the lasso is minimizing an <span class="math">\(\ell_2\)</span> norm plus an <span class="math">\(\ell_1\)</span> norm.</p>
<p>Minimizing the <span class="math">\(\ell_1\)</span> norm (or any optimization problem involving the <span class="math">\(\ell_1\)</span> norm) requires the use of various numerical analysis <a href="https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf">algorithms</a>. These were computationally unfeasible until recent developments in computing power.</p>
<p>Minimizing the <span class="math">\(\ell_2\)</span> norm is a more simple prospect. The author of the blog presents a way of doing this involving Lagrangian multipliers that ultimately dumps us off in OLS land. First, note that our objective function is</p>
<div class="math">$$ \min ||x|\vert_2 \text{ s.t. } Ax = b  $$</div>
<p>The Lagrangian for this is:</p>
<div class="math">$$ ||x||^2_2 + \lambda' (Ax - b)$$</div>
<p>After solving<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup>, the minimized estimate of x is <span class="math">\(A' (AA')^{-1} b\)</span>. The 'a-ha' moment for me, was recognizing that this is in fact the same as <span class="math">\(\hat{\beta} = (X'X)^{-1}X'y\)</span>. The <span class="math">\((X'X)^{-1}X'\)</span> is known as the <a href="https://en.wikipedia.org/wiki/Mooreâ€“Penrose_pseudoinverse">Moore-Penrose Pseudoinverse Matrix</a>.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p><a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov Regularization</a>&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>The mechanics of this process in the context of OLS (called constrained least squares) can be seen <a href="http://stanford.edu/class/ee103/lectures/constrained-least-squares/constrained-least-squares_slides.pdf">here</a>&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/linear-algebra.html">linear-algebra</a>
      <a href="./tag/regression.html">regression</a>
      <a href="./tag/math.html">math</a>
    </p>
  </div>




</article>

    <footer>
<p>&copy; kyle thomas </p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " cmd+build ",
  "url" : ".",
  "image": "",
  "description": ""
}
</script>
</body>
</html>